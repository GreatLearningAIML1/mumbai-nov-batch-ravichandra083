{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Questions_Hyd_Nov18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyfMmMnPJjvn"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjcGOJhcJjvp"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jR0Pl2XjJjvq"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qr75v_UYJjvs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "74634c67-9984-4fd3-8880-f4ba1d3d3755"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCE-3QX2nCwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTI42-0qJjvw"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2sf67VoJjvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0eeecae3-d9d6-4395-fa20-064eb89de7da"
      },
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zewyDcBlJjv1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WytT2eRnJjv4"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XycQGBSGJjv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53722825-4378-4523-f7f7-f5487479fb8f"
      },
      "source": [
        "x_train.shape[1:]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jtdZ7RqJjv8"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAD3q5I6Jjv9",
        "colab": {}
      },
      "source": [
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xO5BRBzBJjwD"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3fUQpMHxJjwE",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "da5-DwgrJjwM"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LPGVQ-JJJjwN",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFRRTJq8JjwQ"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dWTZYnKSJjwR",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DORCLgSwJjwV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ffe188ed-0f22-4e2c-9b34-feed75275f9b"
      },
      "source": [
        "model=Sequential()\n",
        "\n",
        "# First conv layer\n",
        "\n",
        "model.add(Convolution2D(32,3,3,input_shape=(28,28,1)))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#second conv layer\n",
        "\n",
        "model.add(Convolution2D(32,3,3,))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#fully connected layer\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(10))\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "early_stops=keras.callbacks.EarlyStopping(patience=5)\n",
        "\n",
        "call_backs=[early_stops]\n",
        "\n",
        "model.fit(x_train,y_train,batch_size=32,nb_epoch=10,validation_data=(x_test, y_test), callbacks=call_backs)\n",
        "\n",
        " \n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 11s 184us/step - loss: 0.3808 - acc: 0.8632 - val_loss: 0.3108 - val_acc: 0.8865\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.2335 - acc: 0.9134 - val_loss: 0.2617 - val_acc: 0.9048\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.1666 - acc: 0.9382 - val_loss: 0.2513 - val_acc: 0.9125\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.1155 - acc: 0.9569 - val_loss: 0.2770 - val_acc: 0.9136\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.0765 - acc: 0.9718 - val_loss: 0.3299 - val_acc: 0.9085\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0507 - acc: 0.9815 - val_loss: 0.3768 - val_acc: 0.9131\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 10s 171us/step - loss: 0.0368 - acc: 0.9867 - val_loss: 0.4420 - val_acc: 0.9080\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 10s 170us/step - loss: 0.0282 - acc: 0.9897 - val_loss: 0.4672 - val_acc: 0.9096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f95010097f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C18AoS7eJjwU"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ju69vKdIJjwX"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2hAP94vJjwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "91751ac2-9a6b-48fe-d7f1-1f5cfd7ad738"
      },
      "source": [
        "model=Sequential()\n",
        "\n",
        "# First conv layer\n",
        "\n",
        "model.add(Convolution2D(32,3,3,input_shape=(28,28,1)))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "#second conv layer\n",
        "\n",
        "model.add(Convolution2D(32,3,3,))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Max Pooling\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "# Dropout\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        " #fully connected layer\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        " \n",
        "model.add(Dense(10))\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        " \n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "early_stops=keras.callbacks.EarlyStopping(patience=5)\n",
        "\n",
        "call_backs=[early_stops]\n",
        "\n",
        "model.fit(x_train,y_train,batch_size=32,nb_epoch=10,validation_data=(x_test, y_test), callbacks=call_backs)\n",
        "\n",
        " \n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 9s 150us/step - loss: 0.3965 - acc: 0.8567 - val_loss: 0.3015 - val_acc: 0.8881\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 8s 136us/step - loss: 0.2597 - acc: 0.9044 - val_loss: 0.2567 - val_acc: 0.9071\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 8s 136us/step - loss: 0.2115 - acc: 0.9214 - val_loss: 0.2366 - val_acc: 0.9134\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.1796 - acc: 0.9326 - val_loss: 0.2283 - val_acc: 0.9183\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.1527 - acc: 0.9432 - val_loss: 0.2658 - val_acc: 0.9111\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 8s 136us/step - loss: 0.1334 - acc: 0.9509 - val_loss: 0.2172 - val_acc: 0.9242\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.1131 - acc: 0.9575 - val_loss: 0.2643 - val_acc: 0.9164\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.0981 - acc: 0.9632 - val_loss: 0.2533 - val_acc: 0.9234\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 8s 139us/step - loss: 0.0884 - acc: 0.9669 - val_loss: 0.2588 - val_acc: 0.9241\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 8s 138us/step - loss: 0.0753 - acc: 0.9718 - val_loss: 0.2869 - val_acc: 0.9225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9500af7518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lGTA3bfEJjwa"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F6gX8n5SJjwb"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cbz4uHBuJjwc",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    zca_whitening=False,  # apply ZCA whitening\n",
        "    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Prepare the generator\n",
        "datagen.fit(x_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl-8dOo7Jjwf"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpI1_McYJjwg",
        "outputId": "0643a546-163e-4765-d1a0-b6931bca3f33",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n03fYAuwpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmPl5yE8Jjwm"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44ZnDdJYJjwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "ffdc92c3-2725-4394-ae2a-70d58c8091ad"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train,y_train,\n",
        "                                batch_size=32),\n",
        "                    samples_per_epoch=x_train.shape[0],\n",
        "                    nb_epoch=10,\n",
        "                    validation_data=(x_test, y_test), callbacks=call_backs)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=1875, epochs=10)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 20s 10ms/step - loss: 0.7768 - acc: 0.7210 - val_loss: 0.3387 - val_acc: 0.8813\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.5383 - acc: 0.7987 - val_loss: 0.3840 - val_acc: 0.8708\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.4787 - acc: 0.8228 - val_loss: 0.3235 - val_acc: 0.8874\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.4485 - acc: 0.8337 - val_loss: 0.3166 - val_acc: 0.8903\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.4290 - acc: 0.8410 - val_loss: 0.3181 - val_acc: 0.8876\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.4107 - acc: 0.8491 - val_loss: 0.3388 - val_acc: 0.8796\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3980 - acc: 0.8528 - val_loss: 0.3028 - val_acc: 0.8901\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3826 - acc: 0.8575 - val_loss: 0.3332 - val_acc: 0.8832\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 19s 10ms/step - loss: 0.3785 - acc: 0.8594 - val_loss: 0.3006 - val_acc: 0.8935\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 18s 10ms/step - loss: 0.3732 - acc: 0.8626 - val_loss: 0.3137 - val_acc: 0.8934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9500385898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MwQQW5iOJjwq"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c1SrtBEPJjwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "33b12045-c2ac-499b-d8fd-88593ffe3782"
      },
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 55us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31371306624412537, 0.8934]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBwVWNQC2qZD",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}